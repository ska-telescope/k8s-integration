image: nexus.engageska-portugal.pt/ska-docker/deploy:0.4.3

variables:
  DEPLOY_ENVIRONMENT: "test"
  CT_TEST_ENVIRONMENT: "ct-test"
  INGRESS_HOST: "integration.engageska-portugal.pt"
  DEPLOY_URL: "http://integration.engageska-portugal.pt/testdb"
  HELM_TILLER_SILENT: "true"
  HELM_TILLER_LOGS: "true"
  VALUES: "pipeline.yaml"
  MARK: "all"
  MINIKUBE: "false"

stages:
  - static_analysis
  - unit_test
  - clean
  - deploy
  - test
  - post_test
  - debug

.pip_install_test_requirements: &pip_install_test_requirements |-
  cd pre-deployment
  pip3 install -r test-requirements.txt

.teardown_k8s_test_resources: &teardown_k8s_test_resources |-
  kubectl delete all --all --namespace=ci-$CI_JOB_ID || true
  kubectl delete namespace ci-$CI_JOB_ID || true
  helm ls | grep -E '(FAILED|DELETED)' | awk '{print $1}' | xargs -I'%N' helm delete --purge \%N

before_script:
  - export HELM_TILLER_LOGS_DIR=$PWD/tiller.log

helm-lint:
  stage: static_analysis
  tags:
  - docker-executor
  allow_failure: true
  script:
  - make lint_all

.ct-lint:
  stage: static_analysis
  tags:
  - docker-executor
  allow_failure: true
  variables:
    PYTHONIOENCODING: utf8
  script:
  - ct lint --config ci/ct.yaml --all

.ct-test:
  stage: test
  tags:
  - docker-executor
  allow_failure: true
  script:
  - ct install --config ci/ct.yaml --release-label $CI_COMMIT_REF_SLUG --build-id $CI_JOB_ID --namespace ct-test-$CI_JOB_ID
  after_script:
    - kubectl delete namespace ct-test-$CI_JOB_ID || true
  artifacts:
    when: always
    paths:
    - tiller.log
  environment:
    name: $CT_TEST_ENVIRONMENT

.chart-pytest:
  stage: test
  tags:
    - docker-executor
  allow_failure: true
  before_script:
    - *pip_install_test_requirements
  script:
    - pytest -m "chart_deploy and not quarantine" --use-tiller-plugin --test-namespace=ci-$CI_JOB_ID || [ "$?" == "5" ] && true
  after_script:
    - *teardown_k8s_test_resources
  environment:
    name: $CT_TEST_ENVIRONMENT
  artifacts:
    when: always
    name: chart-pytest-reports
    paths:
    - junit-report.xml
    - pytest-logs.txt
    - tiller.log
    reports:
      junit: junit-report.xml

.chart-pytest-quarantine:
  stage: test
  variables:
    KUBE_NAMESPACE: "integration"
  tags:
    - docker-executor
  allow_failure: true
  before_script:
    - *pip_install_test_requirements
  script:
    - pytest -m quarantine --use-tiller-plugin --test-namespace=ci-$CI_JOB_ID || true
  after_script:
    - *teardown_k8s_test_resources
  environment:
    name: $CT_TEST_ENVIRONMENT
  artifacts:
    when: always
    name: chart-pytest-reports
    paths:
      - junit-report.xml
      - pytest-logs.txt
    reports:
      junit: junit-report.xml

uninstall chart:
  stage: clean
  variables:
    KUBE_NAMESPACE: "integration"
  tags:
  - docker-executor
  script:
  - kubectl -n $KUBE_NAMESPACE delete pods,svc,daemonsets,deployments,replicasets,statefulsets,cronjobs,jobs,ingresses,etcd,configmaps --all
  - make uninstall
  environment:
    name: $DEPLOY_ENVIRONMENT
    kubernetes:
      namespace: $KUBE_NAMESPACE
  only:
    refs:
      - master
    variables:
      - $DELETE == "true"

install or upgrade chart:
  stage: deploy
  variables:
    KUBE_NAMESPACE: "integration"
  tags:
  - docker-executor
  script:
  - make install || make upgrade-chart
  - kubectl get all,pv,pvc,ingress -n $KUBE_NAMESPACE
  environment:
    name: $DEPLOY_ENVIRONMENT
    url: $DEPLOY_URL
    kubernetes:
      namespace: $KUBE_NAMESPACE
  only:
    refs:
      - master

k8s_test test environment:
  stage: test
  variables:
    KUBE_NAMESPACE: "integration"
  retry: 2
  tags:
  - docker-executor
  script:
  - make k8s_test && [ -f "build/report.xml" ]
  environment:
    name: $DEPLOY_ENVIRONMENT
    url: $DEPLOY_URL
    kubernetes:
      namespace: $KUBE_NAMESPACE
  only:
    refs:
      - master
  artifacts:
    name: "$CI_PROJECT_NAME-$CI_JOB_ID"
    paths:
      - "build/"
    reports:
      junit: build/report.xml
    when: always

smoketest test environment:
  variables:
    KUBE_NAMESPACE: "integration"
  stage: test
  tags:
  - docker-executor
  script:
  - make smoketest
  environment:
    name: $DEPLOY_ENVIRONMENT
    url: $DEPLOY_URL
    kubernetes:
      namespace: $KUBE_NAMESPACE
  only:
    refs:
      - master


install or upgrade chart on demand:
  stage: deploy
  variables:
    HELM_RELEASE: "test-$CI_COMMIT_SHORT_SHA"
    DOMAIN_TAG: "test-$CI_COMMIT_SHORT_SHA"
  tags:
  - docker-executor
  script:
  - make install || make upgrade-chart
  - kubectl get all,pv,pvc,ingress -n ci-$CI_PROJECT_NAME-$CI_COMMIT_SHORT_SHA
  environment:
    name: $DEPLOY_ENVIRONMENT
    url: $DEPLOY_URL
    kubernetes:
      namespace: ci-$CI_PROJECT_NAME-$CI_COMMIT_SHORT_SHA
  when: manual
  only:
    refs:
      - branches


k8s_test test environment on demand:
  stage: test
  variables:
    HELM_RELEASE: "test-$CI_COMMIT_SHORT_SHA"
    DOMAIN_TAG: "test-$CI_COMMIT_SHORT_SHA"
  retry: 2
  tags:
  - docker-executor
  script:
  - make k8s_test && [ -f "build/report.xml" ]
  after_script:
    - make delete_namespace
  environment:
    name: $DEPLOY_ENVIRONMENT
    url: $DEPLOY_URL
    kubernetes:
      namespace: ci-$CI_PROJECT_NAME-$CI_COMMIT_SHORT_SHA
  when: manual
  only:
    refs:
      - branches
  artifacts:
    name: "$CI_PROJECT_NAME-$CI_JOB_ID"
    paths:
      - "build/"
    reports:
      junit: build/report.xml
    when: always

smoketest test environment on demand:
  stage: test
  variables:
    HELM_RELEASE: "test-$CI_COMMIT_SHORT_SHA"
    DOMAIN_TAG: "test-$CI_COMMIT_SHORT_SHA"
  tags:
  - docker-executor
  script:
  - make smoketest
  environment:
    name: $DEPLOY_ENVIRONMENT
    url: $DEPLOY_URL
    kubernetes:
      namespace: ci-$CI_PROJECT_NAME-$CI_COMMIT_SHORT_SHA
  when: manual
  only:
    refs:
      - branches

sdp_config_dump on demand:
  stage: debug
  variables:
    HELM_RELEASE: "test-$CI_COMMIT_SHORT_SHA"
    DOMAIN_TAG: "test-$CI_COMMIT_SHORT_SHA"
  when: manual
  only:
    refs:
      - branches
  tags:
  - docker-executor
  script:
  - kubectl exec -n ci-$CI_PROJECT_NAME-$CI_COMMIT_SHORT_SHA sdp-proto-console-0 -- sdpcfg ls values -R / > sdp.config.json
  environment:
    name: $DEPLOY_ENVIRONMENT
    kubernetes:
      namespace: ci-$CI_PROJECT_NAME-$CI_COMMIT_SHORT_SHA
  artifacts:
    paths:
    - sdp.config.json


.run-debug-cmd:
  stage: debug
  variables:
    DEBUG_CMD: echo noop
  tags:
  - docker-executor
  script:
  - /bin/sh -exc "$DEBUG_CMD"
  when: manual
  allow_failure: true

print-debug-info:
  stage: debug
  variables:
    KUBE_NAMESPACE: "integration"
  when: always
  tags:
  - docker-executor
  script:
  - mkdir .public
  - kubectl describe ingresses --all-namespaces > .public/ingresses-all-namespaces.txt
  - kubectl describe -n kube-system pod $(kubectl get pod -n kube-system | grep traefik | cut -d\  -f1) > .public/describe-traefik.txt
  - kubectl logs --tail=100 -n kube-system  $(kubectl get pod -n kube-system | grep traefik | cut -d\  -f1) > .public/logs-traefik.txt
  - make podlogs > .public/podlogs.txt
  - kubectl logs --tail=100 -n $KUBE_NAMESPACE $(kubectl get pod -n $KUBE_NAMESPACE | grep fluentd | head -n 1  | cut -d\  -f1) > .public/fluentd.txt
  - kubectl get all --all-namespaces > .public/get-all-namespaces.txt
  - mv .public public
  environment:
    name: $DEPLOY_ENVIRONMENT
    kubernetes:
      namespace: $KUBE_NAMESPACE
  when: manual
  artifacts:
    paths:
    - public

sdp_config_dump:
  stage: debug
  variables:
    KUBE_NAMESPACE: "integration"
  only:
    refs:
      - master
  tags:
  - docker-executor
  script:
  - kubectl exec -n $KUBE_NAMESPACE sdp-proto-console-0 -- sdpcfg ls values -R / > sdp.config.json
  environment:
    name: $DEPLOY_ENVIRONMENT
    kubernetes:
      namespace: $KUBE_NAMESPACE
  artifacts:
    paths:
    - sdp.config.json

xray_report:
  stage: post_test
  tags:
  - docker-executor
  script:
    - 'curl -X POST -H "Content-type: application/json" --fail
         -H "Authorization: Basic $JIRA_AUTH"
         --data @build/cucumber.json
         https://jira.skatelescope.org/rest/raven/1.0/import/execution/cucumber'
  when: always
  only: [master]
  retry: 2
